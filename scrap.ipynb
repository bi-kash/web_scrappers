{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import random\n",
    "import openpyxl\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_driver():\n",
    "    # read proxy list from proxy_list.txt and save it in a list. Also remove line that is empty\n",
    "    with open(\"proxy_list.txt\", \"r\") as file:\n",
    "        proxy_list = file.read().splitlines()\n",
    "        proxy_list = [x for x in proxy_list if x]\n",
    "\n",
    "\n",
    "\n",
    "    chromeOptions = webdriver.ChromeOptions()\n",
    "\n",
    "    # Headless is faster. If headless is False then it opens a browser and you can see action of web driver. You can try making it False\n",
    "    chromeOptions.headless = False\n",
    "    chromeOptions.add_argument(\"--log-level=3\")\n",
    "\n",
    "    # use rotating proxy. Configuration geo.iproyal allows for rotating proxy servers automatically. If you do not have proxy then increase sleep time in random_sleep function\n",
    "    '''\n",
    "    proxy = random.choice(proxy_list)\n",
    "    print(f\"Using proxy: {proxy}\")\n",
    "    chromeOptions.add_argument('--proxy-server={}'.format(proxy))\n",
    "    '''\n",
    "    # installs chrome driver automatically if not present\n",
    "    s = Service(ChromeDriverManager().install())\n",
    "\n",
    "    driver = webdriver.Chrome(\n",
    "        service=Service(ChromeDriverManager().install()), options=chromeOptions\n",
    "    )\n",
    "    return driver\n",
    "\n",
    "def remove_matching_entity_ids(scraped_entity_ids, all_entity_ids):\n",
    "    # remove matching entity ids\n",
    "    for entity_id in scraped_entity_ids:\n",
    "        all_entity_ids.remove(entity_id.replace(\"-\", \"\"))\n",
    "    return all_entity_ids[:500]\n",
    "\n",
    "def random_sleep(min_sec=5, max_sec=12):\n",
    "    # avoiding ban\n",
    "    time.sleep(random.randint(min_sec, max_sec))\n",
    "\n",
    "def entity_id_list():\n",
    "    result = []\n",
    "    for i in range(965):  # 000 to 964\n",
    "        for j in range(1000):  # 000 to 999\n",
    "            for k in range(1, 1000):  # 000 to 999\n",
    "                current_string = f\"{i:03d}{j:03d}{k:03d}\"\n",
    "                result.append(current_string)\n",
    "                if current_string == \"000964437\":\n",
    "                    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'result.xlsx'\n",
    "\n",
    "entity_id_list = entity_id_list()\n",
    "records = []\n",
    "if os.path.exists(filename):\n",
    "    df = pd.read_excel(filename)\n",
    "    records = df.to_dict(orient='records')\n",
    "    scraped_entity_ids = df[\"Entity ID Number\"].tolist()\n",
    "    # remove matching entity ids\n",
    "    entity_id_list = remove_matching_entity_ids(scraped_entity_ids, entity_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_scrap(entity_id_list):\n",
    "    # we have to visit each new link to scrap detaily so it takes time.\n",
    "    url = \"https://arc-sos.state.al.us/cgi/corpdetail.mbr/detail?corp=\"\n",
    "    driver = get_driver()\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    for entity_id in entity_id_list:\n",
    "        try:\n",
    "            '''\n",
    "            count += 1\n",
    "            if count % 10 == 0:\n",
    "                driver.close()\n",
    "                driver = get_driver()\n",
    "            '''\n",
    "            driver.get(url+entity_id)\n",
    "            \n",
    "            if driver.find_elements(By.ID, \"main-frame-error\"):\n",
    "                driver.close()\n",
    "                print(\"Proxy did not work so quit it. Using another proxy\")\n",
    "                driver = get_driver()\n",
    "                driver.get(url+entity_id)\n",
    "                \n",
    "            random_sleep()\n",
    "            info_dict = {}\n",
    "            try:\n",
    "                if driver.find_element(By.ID, \"block-sos-content\").text == 'No matches found.':\n",
    "                    info_dict['Entity ID Number'] = entity_id[:3] + '-' + entity_id[3:6] + '-' + entity_id[6:9]\n",
    "                    info_dict['Status'] = 'No matches found.'\n",
    "                    records.append(info_dict)\n",
    "\n",
    "                    continue\n",
    "                infos = [info.find_elements(By.TAG_NAME, \"td\") for info in driver.find_element(By.TAG_NAME, \"tbody\").find_elements(By.TAG_NAME, \"tr\")]\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            for info in infos:\n",
    "                try:\n",
    "                    key = info[0].text\n",
    "                    value = info[1].text\n",
    "                    info_dict[key] = value\n",
    "                except IndexError:\n",
    "                    pass\n",
    "            print(\"hello\")\n",
    "            \n",
    "            info_dict['Entity Name'] = driver.find_element(By.TAG_NAME, 'td').text # first td is entity name\n",
    "\n",
    "            records.append(info_dict)\n",
    "            if len(records) % 5 == 0:\n",
    "                print(\"Number of records scraped:\", len(records))\n",
    "                pd.DataFrame.from_records(records).to_excel(filename, index=False)\n",
    "        \n",
    "        except:\n",
    "            driver.save_screenshot('screenshot.png')\n",
    "            print(driver.current_url)\n",
    "            print(traceback.format_exc())\n",
    "            driver.close()\n",
    "            driver = get_driver()\n",
    "        \n",
    "    if len(records) > 0:\n",
    "        pd.DataFrame.from_records(records).to_excel(filename, index=False)\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using proxy: 45.251.61.113:6831\n",
      "Using proxy: 137.59.4.152:6021\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if you want to use multithreading use this. But be careful. Simultanus website visit from same IP can lead to ban. So I have avoided it after trying\n",
    "max_workers = 2\n",
    "each_list_len = int(len(entity_id_list)/max_workers)+1\n",
    "list_of_list = []\n",
    "for i in range(max_workers-1):\n",
    "    list_of_list.append(entity_id_list[each_list_len * i : each_list_len* (i + 1)])\n",
    "list_of_list.append(entity_id_list[each_list_len * (i + 1) :])\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    executor.map(detailed_scrap, list_of_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename = 'result.xlsx'\n",
    "df = pd.read_excel(filename).sort_values(by='Entity ID Number')\n",
    "df.to_excel('result_final.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
